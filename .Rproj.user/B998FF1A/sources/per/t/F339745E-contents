# Function to optimize xgboost

optimize_xgboost <- function(
    data = NULL, # data frame, input data
    target = NULL,  # character vector (length 1), target variable.
    cov_names = NULL,  # Character vector, covariate names,
    cov_keep = NULL, # Character vector, covariates that should always be present
    bounds_bayes = NULL, # named list with bounds for bayesian opt.
    bounds_pred = rep(FALSE, 2), # numeric, length 2, bounds for predicted values
    trgrid = NULL, # data frame with tuning parameters to be tested in basic model
    folds = NULL, # list with indices, folds for cross validation
    weights = NULL, # numeric, weights for model training and evaluation
    sumfun = NULL, # summary function for accuracy assessment
    metric = NULL, # character, length 1, name of evaluation metric
    max_metric = NULL, # logical, should the evaluation metric be maximized
    classprob = FALSE, # should class probabilities be calculated
    obj_xgb = NULL, # character, length 1, objective function for xgboost
    trees_per_round = NULL, # numeric, length 1, number of trees that xgboost should train in each round
    colsample_bynode_basic = 0.75, # numeric, colsample_bynode for basic model
    final_round_mult = 1,  # Multiplier for the number of rounds in the final model
    maxd = 10^3, # Maximum depth for optimized models
    cores = 19, # number cores for parallelization
    seed = NULL  # Random seed for model training
) {
  require(ParBayesianOptimization)
  require(caret)
  require(xgboost)
  require(magrittr)
  require(dplyr)
  # Identify OGCs and make a list with the numbers of OGCs to be tested in the
  # models
  ogcs_names <- cov_names %>%
    grep('ogc_pi', ., value = TRUE)
  ogcs_names_list <- list(ogcs_names)
  n_ogcs_v <- numeric()
  m <- 1
  n_ogcs <- length(ogcs_names_list[[m]])
  n_ogcs_v[m] <- n_ogcs
  while (n_ogcs > 2) {
    m <- m + 1
    ogcs_names_list[[m]] <- ogcs_names_list[[m - 1]][c(TRUE, FALSE)]
    n_ogcs <- length(ogcs_names_list[[m]])
    n_ogcs_v[m] <- n_ogcs
  }
  ogcs_names_list[[length(ogcs_names_list) + 1]] <- character()
  n_ogcs_v %<>% c(., 0)
  # Identify covariates that are not OGCs and make formula
  formula_basic <- cov_names %>%
    grep('ogc_pi', ., value = TRUE, invert = TRUE) %>%
    paste0(collapse = " + ") %>%
    paste0(target, " ~ ", .) %>%
    as.formula()
  # Basic model
  showConnections()
  cl <- parallel::makePSOCKcluster(cores, outfile = "log.txt")
  doParallel::registerDoParallel(cl)
  set.seed(seed)
  basic_model <- caret::train(
    form = formula_basic,
    data = data,
    method = "xgbTree",
    na.action = na.pass,
    tuneGrid = trgrid,
    trControl = trainControl(
      index = folds,
      # savePredictions = "final",
      predictionBounds = bounds_pred,
      summaryFunction = sumfun,
      allowParallel = TRUE,
      classProbs = classprob
    ),
    metric = metric,
    maximize = max_metric,
    weights = weights,
    num_parallel_tree = trees_per_round,
    objective = obj_xgb,
    colsample_bynode = colsample_bynode_basic,
    tree_method = "approx",
    nthread = 1
  )
  parallel::stopCluster(cl)
  foreach::registerDoSEQ()
  rm(cl)
  showConnections()
  # Scaled cumulative covariate importance
  cov_ranked <- basic_model %>%
    caret::varImp(basic_model, useModel = TRUE) %>%
    .$importance %>%
    tibble::rownames_to_column() %>%
    dplyr::arrange(desc(Overall)) %>%
    dplyr::mutate(
      scaled = Overall/sum(Overall),
      cumul = cumsum(scaled)
    )
  # Scoring function for Bayesian optimization
  scoringFunction <- function(
    eta,  # OK
    # max_depth,  # OK
    min_child_weight_sqrt,  # OK
    gamma_sqrt,  # OK
    colsample_bytree,  # OK
    subsample,  # OK
    colsample_bynode,
    ogcs_index,  # OK
    total_imp  # OK
  ) {
    # Drop unimportant covariates and add OGCs
    cov_filtered <- cov_ranked %>%
      dplyr::filter(cumul < total_imp) %>%  #!
      .$rowname %>%
      c(., ogcs_names_list[[ogcs_index]])  # !
    # Add permanent covariates
    if (!is.null(cov_keep)) {
      cov_filtered %<>%
        c(., cov_keep) %>%
        unique()
    }
    # Make formula
    formula_i <- cov_filtered %>%
      paste0(collapse = " + ") %>%
      paste0(target, " ~ ", .) %>%
      as.formula()
    my_gamma <- gamma_sqrt^2
    my_min_child_weight <- min_child_weight_sqrt^2
    # Train model
    set.seed(seed)
    model_out <- caret::train(
      form = formula_i,
      data = data,
      method = "xgbTree",
      na.action = na.pass,
      tuneGrid = expand.grid(
        nrounds = trgrid$nrounds,
        eta = eta,  # !
        max_depth = maxd,  # !
        min_child_weight = my_min_child_weight, # !
        gamma = my_gamma, # !
        colsample_bytree = colsample_bytree, # !
        subsample = subsample # !
      ),
      trControl = caret::trainControl(
        index = folds,
        predictionBounds = bounds_pred,
        summaryFunction = sumfun,
        allowParallel = FALSE,
        classProbs = classprob
      ),
      metric = metric,
      maximize = max_metric,
      weights = weights,
      num_parallel_tree = trees_per_round,
      objective = obj_xgb,
      colsample_bynode = colsample_bynode,
      nthread = 1,
      tree_method = "approx"
      # ,
      # grow_policy = "lossguide"
    )
    if (max_metric) {
      out_score <- model_out$results %>%
        dplyr::select(any_of(metric)) %>%
        max()
    } else {
      out_score <- model_out$results %>%
        dplyr::select(any_of(metric)) %>%
        min() %>%
        "*"(-1)
    }
    return(
      list(
        Score = out_score,
        n_ogcs = length(ogcs_names_list[[ogcs_index]]),
        gamma = my_gamma,
        min_child_weight = my_min_child_weight,
        n_cov = length(cov_filtered)
      )
    )
  }
  # Bayesian optimization
  showConnections()
  cl <- parallel::makePSOCKcluster(cores, outfile = "log.txt")
  doParallel::registerDoParallel(cl)
  clusterEvalQ(
    cl,
    {
      require(caret)
      require(xgboost)
      require(magrittr)
      require(dplyr)
      require(tools)
      require(boot)
    }
  )
  clusterExport(
    cl,
    c(
      "target",
      "bounds_pred",
      "cov_ranked",
      "cov_keep",
      "folds",
      "sumfun",
      "metric",
      "maxd",
      "obj_xgb",
      "ogcs_names_list",
      "trgrid",
      "data",
      "trees_per_round",
      "weights",
      "seed"
    ),
    envir = environment()
  )
  set.seed(seed)
  scoreresults <- ParBayesianOptimization::bayesOpt(
    FUN = scoringFunction,
    bounds = bounds_bayes,
    initPoints = cores*2,
    iters.n = cores*10,
    iters.k = cores,
    acq = "ucb",
    gsPoints = cores*10,
    parallel = TRUE,
    verbose = 0,
    acqThresh = 0.95
  )
  stopCluster(cl)
  foreach::registerDoSEQ()
  rm(cl)
  showConnections()
  bestscores <- scoreresults$scoreSummary %>%
    filter(Score == max(Score, na.rm = TRUE))
  best_pars <- getBestPars(scoreresults)
  # Final model
  # Drop unimportant covariates
  cov_filtered <- cov_ranked %>%
    filter(cumul < best_pars$total_imp) %>%  #!
    .$rowname
  # Add permanent covariates
  if (!is.null(cov_keep)) {
    cov_filtered %<>%
      c(., cov_keep) %>%
      unique()
  }
  total_imp <- best_pars$total_imp
  # Add OGCs
  cov_filtered %<>% c(., ogcs_names_list[[best_pars$ogcs_index]])  # !
  n_ogcs_final <- n_ogcs_v[best_pars$ogcs_index]
  # Make formula
  formula_final <- cov_filtered %>%
    paste0(collapse = " + ") %>%
    paste0(target, " ~ ", .) %>%
    as.formula()
  if (final_round_mult > 1) {
    # Lower eta
    eta_test_final <- best_pars$eta %>%
      log() %>%
      seq(., . + log(0.01), length.out = 9) %>%
      exp() %>%
      round(3)
  } else {
    eta_test_final <- best_pars$eta
  }
  showConnections()
  cl <- parallel::makePSOCKcluster(cores, outfile = "log.txt")
  # on.exit(parallel::stopCluster(cl))
  doParallel::registerDoParallel(cl)
  set.seed(seed)
  model_final <- caret::train(
    form = formula_final,
    data = data,
    method = "xgbTree",
    na.action = na.pass,
    tuneGrid = expand.grid(
      nrounds = trgrid$nrounds*final_round_mult,
      eta = eta_test_final, # NB
      max_depth = maxd,
      min_child_weight = best_pars$min_child_weight_sqrt^2,
      gamma = best_pars$gamma_sqrt^2,
      colsample_bytree = best_pars$colsample_bytree,
      subsample = best_pars$subsample
    ),
    trControl = trainControl(
      index = folds,
      savePredictions = "final",
      predictionBounds = bounds_pred,
      summaryFunction = sumfun,
      allowParallel = TRUE,
      classProbs = classprob
    ),
    metric = metric,
    maximize = max_metric,
    weights = weights,
    num_parallel_tree = trees_per_round,
    objective = obj_xgb,
    colsample_bynode = best_pars$colsample_bynode,
    nthread = 1,
    tree_method = "approx"
    # ,
    # grow_policy = "lossguide"
  )
  stopCluster(cl)
  foreach::registerDoSEQ()
  rm(cl)
  return(
    list(
      model = model_final,
      basic_summary = basic_model$results,
      bayes_results = scoreresults,
      best_scores = bestscores
    )
  )
}

# END