!is.na(UTMX),
!is.na(UTMY),
lower > 0,
upper < 200,
!is.na(dhm2015_terraen_10m)
)
# Weighting by depth intervals
print("Calculating weights")
w_interval <- 10
w_increment <- 1
w_startdepth <- 0
w_maxdepth <- 200
w_depths <- seq(w_startdepth, w_maxdepth, w_increment)
w_iterations <- length(w_depths)
w_mat <- matrix(numeric(), nrow = nrow(trdat), ncol = w_iterations)
cm_mat <- matrix(numeric(), nrow = nrow(trdat), ncol = w_iterations)
for (j in 1:w_iterations)
{
upper_j <- w_depths[j] - w_interval
lower_j <- upper_j + w_interval * 2
trdat_ind <- trdat$lower > upper_j & trdat$upper < lower_j
trdat_ind[is.na(trdat_ind)] <- FALSE
trdat_j <- trdat[trdat_ind, ]
trdat_j %<>%
mutate(
thickness = lower - upper,
upper_int = case_when(
upper > upper_j ~ upper,
.default = upper_j
),
lower_int = case_when(
lower < lower_j ~ lower,
.default = lower_j
),
cm_int = case_when(
thickness == 0 ~ 1,
.default = lower_int - upper_int
)
)
cm_mat[trdat_ind, j] <- trdat_j$cm_int
# # Sigma equal to the radius of a circle with an equal area per sample
# sigma_j <- sqrt(43107 / (nrow(trdat_j) * pi)) * 1000
# Use the expected mean density as a baseline
# Do this calculation for the entire area, as a specific calculation for
# wetlands will give too much weight to these areas.
mean_dens_j <- nrow(trdat_j) / (43107 * 10^6)
# For SOC:
# Separate densities for wetlands and uplands
if (frac == "SOC") {
areas <- c(39807, 3299)
w_j <- numeric(nrow(trdat_j))
for (k in 0:1) {
trdat_j_wl_ind <- trdat_j$cwl_10m_crisp == k
trdat_j_wl_ind %<>% { ifelse(is.na(.), FALSE, .) }
trdat_jk <- trdat_j[trdat_j_wl_ind, ]
# Sigma equal to the radius of a circle with an equal area per sample
sigma_jk <- sqrt(areas[k + 1] / (nrow(trdat_jk) * pi)) * 1000
dens_jk <- get_dens(trdat_jk, sigma_jk)
w_j[trdat_j_wl_ind] <- mean_dens_j / dens_jk
}
} else {
# Sigma equal to the radius of a circle with an equal area per sample
sigma_j <- sqrt(43107 / (nrow(trdat_j) * pi)) * 1000
dens_j <- get_dens(trdat_j, sigma_j)
w_j <- mean_dens_j / dens_j
}
w_j[w_j > 1] <- 1
w_mat[trdat_ind, j] <- w_j
}
cm_mat[is.na(cm_mat)] <- 0
cm_sums <- apply(cm_mat, 1, sum)
w_cm_mat <- w_mat*cm_mat
w_cm_sums <- apply(
w_cm_mat,
1,
function(x) {
out <- sum(x, na.rm = TRUE)
return(out)
}
)
w_depth <- w_cm_sums / cm_sums
w_depth[!is.finite(w_depth)] <- 0
# Using the year as a covariate causes the model to identify the SINKS points
# based only on the sampling year, as the campaign took place over only two
# years, which were also underrepresented in the remaining training data.
if (frac == "SOC") {
w_year <- 0.99^(max(trdat$year, na.rm = TRUE) - trdat$year)
w_year %<>% { ifelse(is.na(.), 0, .) }
w_depth %<>% `*`(w_year)
}
weights_objects[[i]] <- list()
weights_objects[[i]]$cm_mat <- cm_mat
weights_objects[[i]]$cm_sums <- cm_sums
weights_objects[[i]]$w_cm_mat <- w_cm_mat
weights_objects[[i]]$w_cm_sums <- w_cm_sums
weights_objects[[i]]$w_depth <- w_depth
trdat$w <- w_depth
trdat_w_indices <- which(obs$ID_new %in% trdat$ID_new)
weights_objects[[i]]$indices <- trdat_w_indices
models_weights[trdat_w_indices, i] <- w_depth
# Three folds (placeholder)
trdat %<>% mutate(
fold = ceiling(fold / 3)
)
trdat %<>% filter(fold < 4)
if (!use_all_points) {
set.seed(1)
trdat %<>% sample_n(n)
}
trdat_indices <- which(obs$ID_new %in% trdat$ID_new)
holdout_i <- obs[-trdat_indices, ]
holdout_indices <- which(obs$ID_new %in% holdout_i$ID_new)
# List of folds
folds_i <- lapply(
unique(trdat$fold),
function(x) {
out <- trdat %>%
mutate(
is_j = fold != x,
rnum = row_number(),
ind_j = is_j * rnum
) %>%
filter(ind_j != 0) %>%
dplyr::select(., ind_j) %>%
unlist() %>%
unname()
}
)
showConnections()
# Add depth boundaries and methods as covariates
cov_c_i <- cov_selected %>%
c("upper", "lower")
if (i %in% 1:4) {
cov_c_i <- cov_selected %>%
c("upper", "lower") %>%
c(., "SOM_removed")
}
# Identify covariates that are not OGCs
covs_not_ogc <- grep('ogc_pi', cov_c_i, value = TRUE, invert = TRUE)
cov_p_i <- covs_not_ogc %>% paste0(collapse = " + ")
formula_i <- paste0(frac, " ~ ", cov_p_i) %>%
as.formula()
# xgboost optimization
# 1: Fit learning rate (eta)
print("Step 1: Fit learning rate (eta)")
set.seed(1)
models[[i]] <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = tgrid$nrounds,
eta = eta_test, # NB
max_depth = tgrid$max_depth,
min_child_weight = tgrid$min_child_weight,
gamma = tgrid$gamma,
colsample_bytree = tgrid$colsample_bytree,
subsample = tgrid$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i],
colsample_bylevel = 0.1
)
models_tr_summaries[[i]][[tr_step]] <- models[[i]]$results
print(models_tr_summaries[[i]][[tr_step]])
tr_step %<>% `+`(1)
# Scaled cumulative covariate importance
cov_i_ranked <- varImp(models[[i]])$importance %>%
rownames_to_column() %>%
mutate(
scaled = Overall/sum(Overall),
cumul = cumsum(scaled)
)
if (extra_tuning_xgb & xgb_opt_stepwise) {
# CS Step 1: Drop unimportant covariates
cov_c_i <- cov_i_ranked %>%
filter(cumul < 0.99) %>%
.$rowname
cov_p_i <- cov_c_i %>% paste0(collapse = " + ")
formula_i <- paste0(frac, " ~ ", cov_p_i) %>%
as.formula()
# xgb opt Step 2: Fit max_depth and min_child_weight
print("Step 2: Fit max_depth and min_child_weight")
set.seed(1)
model2 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = models[[i]]$bestTune$nrounds,
eta = models[[i]]$bestTune$eta,
max_depth = max_depth_test, # NB
min_child_weight = min_child_weight_test, # NB
gamma = models[[i]]$bestTune$gamma,
colsample_bytree = models[[i]]$bestTune$colsample_bytree,
subsample = models[[i]]$bestTune$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
models[[i]] <- model2
models_tr_summaries[[i]][[tr_step]] <- models[[i]]$results
print(models_tr_summaries[[i]][[tr_step]])
tr_step %<>% `+`(1)
# xgb opt Step 3: Tune gamma
print("Step 3: Tune gamma")
set.seed(1)
model3 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = models[[i]]$bestTune$nrounds,
eta = models[[i]]$bestTune$eta,
max_depth = models[[i]]$bestTune$max_depth,
min_child_weight = models[[i]]$bestTune$min_child_weight,
gamma = gamma_test, # NB
colsample_bytree = models[[i]]$bestTune$colsample_bytree,
subsample = models[[i]]$bestTune$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
models[[i]] <- model3
models_tr_summaries[[i]][[tr_step]] <- models[[i]]$results
print(models_tr_summaries[[i]][[tr_step]])
tr_step %<>% `+`(1)
# xgb opt Step 4: Adjust subsampling
print("Step 4: Adjust subsampling")
set.seed(1)
model4 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = models[[i]]$bestTune$nrounds,
eta = models[[i]]$bestTune$eta,
max_depth = models[[i]]$bestTune$max_depth,
min_child_weight = models[[i]]$bestTune$min_child_weight,
gamma = models[[i]]$bestTune$gamma,
colsample_bytree = colsample_bytree_test,
subsample = subsample_test
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
models[[i]] <- model4
models_tr_summaries[[i]][[tr_step]] <- models[[i]]$results
print(models_tr_summaries[[i]][[tr_step]])
tr_step %<>% `+`(1)
# CS Step 2: Decide the optimal number of OGCs
cov_c_i <- varImp(models[[i]])$importance %>%
rownames_to_column() %>%
.$rowname
ogcs_names_list <- list(ogcs_names)
n_ogcs_v <- numeric()
m <- 1
n_ogcs <- length(ogcs_names_list[[m]])
n_ogcs_v[m] <- n_ogcs
while (n_ogcs > 2) {
m <- m + 1
ogcs_names_list[[m]] <- ogcs_names_list[[m - 1]][c(TRUE, FALSE)]
n_ogcs <- length(ogcs_names_list[[m]])
n_ogcs_v[m] <- n_ogcs
}
ogcs_names_list %<>% lapply(., function(x) {c(cov_c_i, x)})
ogcs_names_list[[length(ogcs_names_list) + 1]] <- cov_c_i
n_ogcs_v %<>% c(., 0)
print("Testing OGCs")
models_ogc_test <- ogcs_names_list %>%
lapply(
.,
function(x) {
cov_p_i <- x %>% paste0(collapse = " + ")
formula_i <- paste0(frac, " ~ ", cov_p_i) %>%
as.formula()
set.seed(1)
out <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = models[[i]]$bestTune$nrounds,
eta = models[[i]]$bestTune$eta,
max_depth = models[[i]]$bestTune$max_depth,
min_child_weight = models[[i]]$bestTune$min_child_weight,
gamma = models[[i]]$bestTune$gamma,
colsample_bytree = models[[i]]$bestTune$colsample_bytree,
subsample = models[[i]]$bestTune$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
return(out)
}
)
ogc_results <- models_ogc_test %>% lapply(
., function(x) x$results %>% select(any_of(metrics[i])) %>% min()
) %>%
unlist()
which_ogc_ind <- which.min(ogc_results)
ogc_df <- data.frame(
fraction = frac,
n_ogcs = n_ogcs_v,
acc = ogc_results,
metric = metrics[i]
)
write.table(
ogc_df,
paste0(dir_results, "ogc_acc_", frac, ".csv"),
row.names = FALSE,
col.names = TRUE,
sep = ";"
)
models_tr_summaries[[i]][[tr_step]] <- ogc_df
print(models_tr_summaries[[i]][[tr_step]])
tr_step %<>% `+`(1)
n_ogcs_models[i] <- n_ogcs_v[which_ogc_ind]
models[[i]] <- models_ogc_test[[which_ogc_ind]]
cov_c_i <- varImp(models_ogc_test[[which_ogc_ind]])$importance %>%
rownames_to_column() %>%
.$rowname
cov_p_i <- cov_c_i %>% paste0(collapse = " + ")
formula_i <- paste0(frac, " ~ ", cov_p_i) %>%
as.formula()
# xgb opt Step 5: Increase nrounds, readjust learning rate
print("Step 5")
eta_test_final <- model4$bestTune$eta %>%
log() %>%
seq(., . + log(0.01), length.out = 9) %>%
exp() %>%
round(3)
set.seed(1)
model5 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = models[[i]]$bestTune$nrounds*10,
eta = eta_test_final, # NB
max_depth = models[[i]]$bestTune$max_depth,
min_child_weight = models[[i]]$bestTune$min_child_weight,
gamma = models[[i]]$bestTune$gamma,
colsample_bytree = models[[i]]$bestTune$colsample_bytree,
subsample = models[[i]]$bestTune$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
models[[i]] <- model5
models_tr_summaries[[i]][[tr_step]] <- models[[i]]$results
print(models_tr_summaries[[i]][[tr_step]])
tr_step %<>% `+`(1)
}
# Bayes optimization
if (extra_tuning_xgb & !xgb_opt_stepwise) {
ScoreResult <- bayesOpt(
FUN = scoringFunction,
bounds = bounds,
initPoints = 20,
iters.n = 10,
iters.k = 1,
acq = "ei",
gsPoints = 20,
parallel = FALSE,
verbose = 1,
)
print(
ScoreResult$scoreSummary
)
print(
ScoreResult$scoreSummary[which.max(ScoreResult$scoreSummary$Score), ]
)
best_pars <- getBestPars(ScoreResult)
print("Training final model")
# Drop unimportant covariates
cov_i_filtered <- cov_i_ranked %>%
filter(cumul < best_pars$total_imp) %>%  #!
.$rowname
total_imp_models[i] <- best_pars$total_imp
# Add OGCs
cov_i_filtered %<>% c(., ogcs_names_list[[best_pars$ogcs_index]])  # !
n_ogcs_models[i] <- n_ogcs_v[best_pars$ogcs_index]
# Make formula
cov_formula <- cov_i_filtered %>% paste0(collapse = " + ")
formula_i <- paste0(frac, " ~ ", cov_formula) %>%
as.formula()
# Lower eta
eta_test_final <- best_pars$eta %>%
log() %>%
seq(., . + log(0.01), length.out = 9) %>%
exp() %>%
round(3)
set.seed(1)
model_final <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = tgrid$nrounds*10,
eta = eta_test_final, # NB
max_depth = best_pars$max_depth,
min_child_weight = best_pars$min_child_weight,
gamma = best_pars$gamma,
colsample_bytree = best_pars$colsample_bytree,
subsample = best_pars$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i],
colsample_bylevel = best_pars$colsample_bylevel
)
models[[i]] <- model_final
}
print(models[[i]])
models_predictions[trdat_indices, i] <- models[[i]]$pred %>%
arrange(rowIndex) %>%
distinct(rowIndex, .keep_all = TRUE) %>%
dplyr::select(., pred) %>%
unlist() %>%
unname()
if (i %in% 1:4) {
n_const_i <- 3
} else {
n_const_i <- 2
}
models_predictions[holdout_indices, i] <- predict_passna(
models[[i]],
holdout_i,
n_const = n_const_i
)
saveRDS(
models[[i]],
paste0(dir_results, "/model_", frac, ".rds")
)
}
