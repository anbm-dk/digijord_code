# 7: Make training data
folds <- bind_rows(
dsc_folds,
SEGES_folds,
SINKS_folds,
profiles_folds,
forest_folds
)
names(folds) <- "fold"
extr <- bind_rows(
dsc_extr,
SEGES_extr,
SINKS_extr,
profiles_extr,
forests_extr
)
obs <- cbind(obs_data, extr, folds) %>%
filter(!is.na(UTMX) & !is.na(UTMY))
obs %<>%
rownames_to_column() %>%
mutate(ID_new = rowname, .before = everything()) %>%
select(-rowname)
obs_top <- obs %>%
filter(
upper < 25,
is.finite(fold)
)
obs_prf <- obs %>%
filter(
db == "Profile database",
is.finite(fold)
)
# Make new ID
obs_top_v <- obs_top %>% vect(geom = c("UTMX", "UTMY"))
library(viridisLite)
tiff(
paste0(dir_results, "/obs_map_test", testn, ".tiff"),
width = 15,
height = 10,
units = "cm",
res = 300
)
plot(
obs_top_v, "clay",
breaks = 5, breakby = "cases", col = cividis(5),
cex = 0.2
)
try(dev.off())
try(dev.off())
plot(
obs_top_v, "clay",
breaks = 5, breakby = "cases", col = cividis(5),
cex = 0.4
)
# 8: Set up models
cov_selected <- cov_cats %>%
filter(anbm_use == 1) %>%
dplyr::select(., name) %>%
unlist() %>%
unname()
# Template for custom eval
# evalerror <- function(preds, dtrain) {
#   labels <- getinfo(dtrain, "label")
#   err <- as.numeric(sum(labels != (preds > 0)))/length(labels)
#   return(list(metric = "error", value = err))
# }
# Weighted RMSE
get_RMSEw <- function(d, w) {
sqe <- w * (d[, 1] - d[, 2])^2
msqe <- sum(sqe) / sum(w)
out <- sqrt(msqe)
return(out)
}
# Weighted R^2
get_R2w <- function(d, w) {
require(boot)
out <- boot::corr(d[, 1:2], w)^2
return(out)
}
# Weighted summary function
WeightedSummary <- function(
data,
lev = NULL,
model = NULL,
...) {
out <- numeric()
out[1] <- get_RMSEw(data[, 1:2], data$weights)
out[2] <- get_R2w(data[, 1:2], data$weights)
names(out) <- c("RMSEw", "R2w")
return(out)
}
# Weighted summary function with log transformation
WeightedSummary_log <- function(
data,
lev = NULL,
model = NULL,
...) {
out <- numeric()
data[, 1:2] <- log(data[, 1:2])
data <- data[is.finite(rowSums(data)), ]
out[1] <- get_RMSEw(data[, 1:2], data$weights)
out[2] <- get_R2w(data[, 1:2], data$weights)
names(out) <- c("RMSEw_log", "R2w_log")
return(out)
}
# Weighted summary function with square root transformation
WeightedSummary_sqrt <- function(
data,
lev = NULL,
model = NULL,
...) {
out <- numeric()
data[, 1:2] <- sqrt(data[, 1:2])
data <- data[is.finite(rowSums(data)), ]
out[1] <- get_RMSEw(data[, 1:2], data$weights)
out[2] <- get_R2w(data[, 1:2], data$weights)
names(out) <- c("RMSEw_sqrt", "R2w_sqrt")
return(out)
}
metrics <- rep("RMSEw", length(fractions))
metrics[fractions == "SOC"] <- "RMSEw_log"
metrics[fractions == "CaCO3"] <- "RMSEw_sqrt"
# Function to calculate point densities
qnorm(seq(0.55, 0.95, 0.1), 0, 1)
get_dens <- function(datxy, sig) {
dens_out <- ppp(
datxy$UTMX,
datxy$UTMY,
c(441000, 894000),
c(6049000, 6403000)
) %>%
density(
sigma = sig,
at = "points",
leaveoneout = FALSE
)
attributes(dens_out) <- NULL
return(dens_out)
}
# Tuning grid
tgrid <- expand.grid(
nrounds = 10,
eta = 0.3,
max_depth = 6,
min_child_weight = 1,
gamma = 0,
colsample_bytree = 0.8,
subsample = 0.8
)
eta_test <- seq(0.1, 1, 0.1)
max_depth_test <- seq(1, 20, 3)
min_child_weight_test <- c(1, 2, 4, 8, 16, 32)
gamma_test <- seq(0, 0.5, 0.1)
colsample_bytree_test <- seq(0.6, 0.9, 0.1)
subsample_test <- seq(0.6, 0.9, 0.1)
objectives <- c(rep("reg:squarederror", 4), rep("reg:tweedie", 2))
trees_per_round <- 10
# Small random sample for testing
# Remember to include full dataset in the final model
n <- 1000
# use_all_points <- TRUE
use_all_points <- FALSE
# 9: Train models
n_ogcs_models <- numeric()
# Covariate selection:
# Step 1: Decide the optimal number of OGCs
# Step 2: Drop unimportant covariates
# xgb optimization:
# Step 1: Adjust learning rate
# Step 2: Fit max_depth and min_child_weight
# Step 3: Tune gamma
# Step 4: Adjust subsampling
# extra_tuning_xgb <- TRUE
extra_tuning_xgb <- FALSE
models <- list()
for (i in 1:length(fractions))
{
frac <- fractions[i]
print(frac)
if (metrics[i] == "RMSEw_log") {
sumfun <- WeightedSummary_log
} else {
if (metrics[i] == "RMSEw_sqrt") {
sumfun <- WeightedSummary_sqrt
} else {
sumfun <- WeightedSummary
}
}
trdat <- obs %>%
filter(is.finite(.data[[frac]])) %>%
filter(!is.na(UTMX) & !is.na(UTMY)) %>%
filter(lower > 0, upper < 200)
# Three folds (placeholder)
trdat %<>% mutate(
fold = ceiling(fold / 3)
)
holdout_i <- trdat %>%
filter(fold == 4)
trdat %<>% filter(fold < 4)
if (!use_all_points) {
set.seed(1)
trdat %<>% sample_n(n)
}
# Weighting by depth intervals
w_interval <- 10
w_increment <- 1
w_startdepth <- 0
w_maxdepth <- 200
w_iterations <- round((w_maxdepth - w_startdepth) / w_increment, digits = 0)
w_mat <- matrix(numeric(), nrow = nrow(trdat), ncol = w_iterations)
cmw_mat <- matrix(numeric(), nrow = nrow(trdat), ncol = w_iterations)
for (j in 1:w_iterations)
{
upper_j <- w_startdepth + w_increment * (j - 1) - w_interval
lower_j <- upper_j + w_interval * 2
trdat_ind <- trdat$lower > upper_j & trdat$upper < lower_j
trdat_ind[is.na(trdat_ind)] <- FALSE
trdat_j <- trdat[trdat_ind, ]
trdat_j %<>%
mutate(
thickness = lower - upper,
upper_int = case_when(
upper > upper_j ~ upper,
.default = upper_j
),
lower_int = case_when(
lower < lower_j ~ lower,
.default = lower_j
),
cm_int = lower_int - upper_int,
cm_w_int = case_when(
thickness == 0 ~ 1,
.default = cm_int / (w_interval*2)
)
)
cmw_mat[trdat_ind, j] <- trdat_j$cm_w_int
# Sigma equal to the radius of a circle with an equal area per sample
sigma_j <- sqrt(42951 / (nrow(trdat_j) * pi)) * 1000
# Use the expected mean density as a baseline
mean_dens_j <- nrow(trdat_j) / (42951 * 10^6)
# For SOC:
# Separate densities for wetlands and uplands
if (frac == "SOC") {
dens_j <- numeric(nrow(trdat_j))
for (k in 0:1) {
trdat_j_wl_ind <- trdat_j$cwl_10m_crisp == k
trdat_jk <- trdat_j[trdat_j_wl_ind, ]
dens_j[trdat_j_wl_ind] <- get_dens(trdat_jk, sigma_j)
}
} else {
dens_j <- get_dens(trdat_j, sigma_j)
}
w_j <- mean_dens_j / dens_j
# w_j[w_j > 1] <- 1
w_mat[trdat_ind, j] <- w_j
}
cmw_mat[is.na(cmw_mat)] <- 0
w_mat <- w_mat*cmw_mat
w_depth <- apply(
w_mat,
1,
function(x) {
out <- mean(x, na.rm = TRUE)
return(out)
}
)
w_depth[!is.finite(w_depth)] <- 1
w_depth[w_depth > 1] <- 1
trdat$w <- w_depth
# List of folds
folds_i <- lapply(
unique(trdat$fold),
function(x) {
out <- trdat %>%
mutate(
is_j = fold != x,
rnum = row_number(),
ind_j = is_j * rnum
) %>%
filter(ind_j != 0) %>%
dplyr::select(., ind_j) %>%
unlist() %>%
unname()
}
)
showConnections()
# Covariate selection
cov_c_i <- cov_selected %>%
c("upper", "lower")
if (i %in% 1:4) {
cov_c_i <- cov_selected %>%
c("upper", "lower") %>%
c(., "SOM_removed")
} else {
if (i == 5) {
cov_c_i <- cov_selected %>%
c("upper", "lower") %>%
c(., "year")
}
}
# CS Step 1: Decide the optimal number of OGCs
ogcs_names <- grep('ogc_pi', cov_c_i, value = TRUE)
covs_not_ogc <- grep('ogc_pi', cov_c_i, value = TRUE, invert = TRUE)
ogcs_names_list <- list(ogcs_names)
n_ogcs_v <- numeric()
m <- 1
n_ogcs <- length(ogcs_names_list[[m]])
n_ogcs_v[m] <- n_ogcs
while (n_ogcs > 2) {
m <- m + 1
ogcs_names_list[[m]] <- ogcs_names_list[[m - 1]][c(TRUE, FALSE)]
n_ogcs <- length(ogcs_names_list[[m]])
n_ogcs_v[m] <- n_ogcs
}
ogcs_names_list %<>% lapply(., function(x) {c(covs_not_ogc, x)})
ogcs_names_list[[length(ogcs_names_list) + 1]] <- covs_not_ogc
n_ogcs_v %<>% c(., 0)
print("Testing OGCs")
models_ogc_test <- ogcs_names_list %>%
lapply(
.,
function(x) {
cov_p_i <- x %>% paste0(collapse = " + ")
formula_i <- paste0(frac, " ~ ", cov_p_i) %>%
as.formula()
set.seed(1)
out <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = tgrid,
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
return(out)
}
)
ogc_results <- models_ogc_test %>% lapply(
., function(x) x$results %>% select(any_of(metrics[i])) %>% min()
) %>%
unlist()
which_ogc_ind <- which.min(ogc_results)
ogc_df <- data.frame(
fraction = frac,
n_ogcs = n_ogcs_v,
acc = ogc_results,
metric = metrics[i]
)
write.table(
ogc_df,
paste0(dir_results, "ogc_acc_", frac, ".csv"),
row.names = FALSE,
col.names = TRUE,
sep = ";"
)
n_ogcs_models[i] <- n_ogcs_v[which_ogc_ind]
# CS Step 2: Drop unimportant covariates
cov_c_i <- varImp(models_ogc_test[[which_ogc_ind]])$importance %>%
rownames_to_column() %>%
mutate(scaled = Overall/sum(Overall),
cumul = cumsum(scaled)) %>%
filter(cumul < 0.99) %>%
.$rowname
cov_p_i <- cov_c_i %>% paste0(collapse = " + ")
formula_i <- paste0(frac, " ~ ", cov_p_i) %>%
as.formula()
# xgboost optimization
# 1: Fit learning rate (eta) [and nrounds]
print("Step 1")
set.seed(1)
models[[i]] <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = tgrid$nrounds,
eta = eta_test, # NB
max_depth = tgrid$max_depth,
min_child_weight = tgrid$min_child_weight,
gamma = tgrid$gamma,
colsample_bytree = tgrid$colsample_bytree,
subsample = tgrid$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
# registerDoSEQ()
# rm(cl)
if (extra_tuning_xgb) {
# xgb opt Step 2: Fit max_depth and min_child_weight
print("Step 2")
set.seed(1)
model2 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = models[[i]]$bestTune$nrounds,
eta = models[[i]]$bestTune$eta,
max_depth = max_depth_test, # NB
min_child_weight = min_child_weight_test, # NB
gamma = models[[i]]$bestTune$gamma,
colsample_bytree = models[[i]]$bestTune$colsample_bytree,
subsample = models[[i]]$bestTune$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
# xgb opt Step 3: Tune gamma
print("Step 3")
set.seed(1)
model3 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = models[[i]]$bestTune$nrounds,
eta = models[[i]]$bestTune$eta,
max_depth = models[[i]]$bestTune$max_depth,
min_child_weight = models[[i]]$bestTune$min_child_weight,
gamma = gamma_test, # NB
colsample_bytree = models[[i]]$bestTune$colsample_bytree,
subsample = models[[i]]$bestTune$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
models[[i]] <- model3
# xgb opt Step 4: Adjust subsampling
print("Step 4")
set.seed(1)
model4 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = models[[i]]$bestTune$nrounds,
eta = models[[i]]$bestTune$eta,
max_depth = models[[i]]$bestTune$max_depth,
min_child_weight = models[[i]]$bestTune$min_child_weight,
gamma = models[[i]]$bestTune$gamma,
colsample_bytree = colsample_bytree_test,
subsample = subsample_test
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
models[[i]] <- model4
}
print(models[[i]])
saveRDS(
models[[i]],
paste0(dir_results, "/model_", frac, ".rds")
)
}
w_depth
plot(w_depth)
