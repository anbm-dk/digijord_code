SEGES_extr <- dir_extr %>%
paste0(., "/SEGES_extr.rds") %>%
readRDS()
}
SINKS_extr <- dir_extr %>%
paste0(., "/SINKS_extr.rds") %>%
readRDS()
profiles_extr <- dir_extr %>%
paste0(., "profiles_extr.rds") %>%
readRDS() %>%
right_join(values(profiles_texture)) %>%
select(any_of(cov_names))
SINKS_extr <- dir_extr %>%
paste0(., "/SINKS_extr.rds") %>%
readRDS()
forests_extr <- dir_extr %>%
paste0(., "/forests_extr.rds") %>%
readRDS()
# 6: Merge data
obs_v <- list(dsc, SEGES, SINKS, profiles_texture, forest_samples) %>%
vect()
# Extract mask values for all the observations
mask_LU <- paste0(dir_dat, "/layers/Mask_LU.tif") %>% rast()
mask_LU_extr <- terra::extract(
mask_LU,
obs_v,
ID = FALSE
)
mask_LU_extr %<>% unlist() %>% unname()
obs_data <- obs_v %>%
values() %>%
mutate(
# logSOC = log(SOC),
# logCaCO3 = log(CaCO3),
year = date %>%
as.character() %>%
substr(start = 1, stop = 4) %>%
as.numeric(),
mask_LU = mask_LU_extr
)
# fractions <- c("clay", "silt", "fine_sand", "coarse_sand", "logSOC", "logCaCO3")
fractions_alt <- c("clay", "silt", "fine_sand", "coarse_sand", "SOC", "CaCO3")
fractions <- fractions_alt
fraction_names <- c(
"Clay", "Silt", "Fine sand", "Coarse sand", "SOC", "CaCO3"
)
bounds_lower <- rep(0, 6)
bounds_upper <- rep(100, 6)
# 7: Make training data
folds <- bind_rows(
dsc_folds,
SEGES_folds,
SINKS_folds,
profiles_folds,
forest_folds
)
names(folds) <- "fold"
extr <- bind_rows(
dsc_extr,
SEGES_extr,
SINKS_extr,
profiles_extr,
forests_extr
)
obs <- cbind(obs_data, extr, folds) %>%
filter(
!is.na(UTMX),
!is.na(UTMY),
!is.na(mask_LU),
is.finite(fold)
)
# Make new ID
obs %<>%
rownames_to_column() %>%
mutate(ID_new = rowname, .before = everything()) %>%
select(-rowname)
write.table(
obs,
paste0(dir_results, "observations_texture.csv"),
row.names = FALSE,
col.names = TRUE,
sep = ";"
)
obs_top <- obs %>%
filter(
upper < 30,
lower > 0
)
obs_prf <- obs %>%
filter(
db == "Profile database"
)
obs_top_v <- obs_top %>% vect(geom = c("UTMX", "UTMY"))
library(tidyterra)
my_breaks <- function(x) {
x <- unique(x)
n <- 6
i <- seq(0, 1, length.out = n)
breaks <- quantile(x, i, na.rm = TRUE)
breaks <- round(breaks, digits = 1)
breaks <- unique(breaks)
if ((breaks[1] %% 1) != 0) {
breaks[1] <- breaks[1] - 0.000001
}
if ((breaks[n] %% 1) != 0) {
breaks[n] <- breaks[n] + 0.000001
}
return(breaks)
}
set.seed(1)
obs_top_plot <- obs_top_v %>%
mutate(random = runif(length(.))) %>%
arrange(random) %>%
select(any_of(fractions))
frac_labels = c(
expression("Clay"~"(%)"),
expression("Silt"~"(%)"),
expression("Fine"~"sand"~"(%)"),
expression("Coarse"~"sand"~"(%)"),
expression("SOC"~"(%)"),
expression("CaCO"[3]~"(%)")
)
library(viridisLite)
mycolors <- cividis(6) %>% rev() %>% .[-1] %>% rev()
tiff(
paste0(dir_results, "/obs_map_test", testn, ".tiff"),
width = 16,
height = 10,
units = "cm",
res = 300
)
par(oma = c(2, 2, 0, 1))
plot(
obs_top_plot,
y = 1:length(fractions),
breaks = 5,
breakby = my_breaks,
col = viridis(5),
colNA = NA,
cex = 0.1,
mar = c(0, 0, 1, 0),
main = frac_labels,
plg = list(
x = "topright", text.col = viridis(5),
fill = viridis(5),
text.font =
2)
)
try(dev.off())
try(dev.off())
par()
# 8: Set up models
cov_selected <- cov_cats %>%
filter(anbm_use == 1) %>%
dplyr::select(., name) %>%
unlist() %>%
unname()
# Template for custom eval
# evalerror <- function(preds, dtrain) {
#   labels <- getinfo(dtrain, "label")
#   err <- as.numeric(sum(labels != (preds > 0)))/length(labels)
#   return(list(metric = "error", value = err))
# }
source("f_weighted_summaries.R")
metrics <- rep("RMSEw", length(fractions))
metrics[fractions == "SOC"] <- "RMSEw_log"
metrics[fractions == "CaCO3"] <- "RMSEw_sqrt"
# Function to calculate point densities
qnorm(seq(0.55, 0.95, 0.1), 0, 1)
source("f_get_dens.R")
# Parameters for weights calculations
w_interval <- 10
w_increment <- 1
w_startdepth <- 0
w_maxdepth <- 200
w_depths <- seq(w_startdepth, w_maxdepth, w_increment)
w_iterations <- length(w_depths)
# Tuning grid
tgrid <- expand.grid(
nrounds = 10,
eta = 0.3,
max_depth = 6,
min_child_weight = 1,
gamma = 0,
colsample_bytree = 0.75,
subsample = 0.75
)
eta_test <- seq(0.1, 1, 0.1)
max_depth_test <- seq(1, 30, 3)
min_child_weight_test <- c(1, 2, 4, 8, 16, 32, 64)
gamma_test <- seq(0, 0.6, 0.1)
colsample_bytree_test <- seq(0.1, 1, 0.1)
subsample_test <- seq(0.1, 1, 0.1)
objectives <- c(rep("reg:squarederror", 4), rep("reg:tweedie", 2))
trees_per_round <- 10
# Bayesian optimization
library(ParBayesianOptimization)
source("f_optimize_xgboost.R")
bounds <- list(
eta = c(0.1, 1),
# max_depth = c(1L, 60L),  # Just set max_depth a very large value
min_child_weight_sqrt = c(1, sqrt(64)),
gamma_sqrt = c(0, sqrt(40)),
colsample_bytree = c(0.1, 1),
subsample = c(0.1, 1),
colsample_bylevel = c(0.1, 1),
ogcs_index = c(1L, 7L),
total_imp = c(0.5, 1)
)
xgb_opt_stepwise <- FALSE
# Small random sample for testing
# Remember to include full dataset in the final model
n <- 1000
# use_all_points <- TRUE
use_all_points <- FALSE
# 9: Train models
if (train_models) {
n_ogcs_models <- numeric()
total_imp_models <- numeric()
weights_objects <- list()
models_tr_summaries <- list()
models_scoreresults <- list()
models_bestscores <- list()
models_predictions <- matrix(
numeric(),
nrow = nrow(obs),
ncol = length(fractions)
)
models_weights <- matrix(
numeric(),
nrow = nrow(obs),
ncol = length(fractions)
)
models_indices <- matrix(
numeric(),
nrow = nrow(obs),
ncol = length(fractions)
)
colnames(models_predictions) <- fractions
colnames(models_weights) <- fractions
models <- list()
for (i in 1:length(fractions))
{
frac <- fractions[i]
print(frac)
tr_step <- 1
if (metrics[i] == "RMSEw_log") {
sumfun_i <- WeightedSummary_log
} else {
if (metrics[i] == "RMSEw_sqrt") {
sumfun_i <- WeightedSummary_sqrt
} else {
sumfun_i <- WeightedSummary
}
}
trdat <- obs %>%
filter(
is.finite(.data[[frac]]),
!is.na(UTMX),
!is.na(UTMY),
lower > 0,
(upper < 200) | (upper == 200 & lower == 200),
!is.na(dhm2015_terraen_10m)
)
# Weighting by depth intervals
print("Calculating weights")
w_mat <- matrix(numeric(), nrow = nrow(trdat), ncol = w_iterations)
cm_mat <- matrix(numeric(), nrow = nrow(trdat), ncol = w_iterations)
for (j in 1:w_iterations)
{
upper_j <- w_depths[j] - w_interval
lower_j <- upper_j + w_interval * 2
trdat_ind <- trdat$lower > upper_j & trdat$upper < lower_j
trdat_ind[is.na(trdat_ind)] <- FALSE
trdat_j <- trdat[trdat_ind, ]
trdat_j %<>%
mutate(
thickness = lower - upper,
upper_int = case_when(
upper > upper_j ~ upper,
.default = upper_j
),
lower_int = case_when(
lower < lower_j ~ lower,
.default = lower_j
),
cm_int = case_when(
thickness == 0 ~ 1,
.default = lower_int - upper_int
)
)
cm_mat[trdat_ind, j] <- trdat_j$cm_int
# # Sigma equal to the radius of a circle with an equal area per sample
# sigma_j <- sqrt(43107 / (nrow(trdat_j) * pi)) * 1000
# Use the expected mean density as a baseline
# Do this calculation for the entire area, as a specific calculation for
# wetlands will give too much weight to these areas.
mean_dens_j <- nrow(trdat_j) / (43107 * 10^6)
# For SOC:
# Separate densities for wetlands and uplands
if (frac == "SOC") {
areas <- c(39807, 3299)
w_j <- numeric(nrow(trdat_j))
for (k in 0:1) {
trdat_j_wl_ind <- trdat_j$cwl_10m_crisp == k
trdat_j_wl_ind %<>% { ifelse(is.na(.), FALSE, .) }
trdat_jk <- trdat_j[trdat_j_wl_ind, ]
# Sigma equal to the radius of a circle with an equal area per sample
sigma_jk <- sqrt(areas[k + 1] / (nrow(trdat_jk) * pi)) * 1000
dens_jk <- get_dens(trdat_jk, sigma_jk)
w_j[trdat_j_wl_ind] <- mean_dens_j / dens_jk
}
} else {
# Sigma equal to the radius of a circle with an equal area per sample
sigma_j <- sqrt(43107 / (nrow(trdat_j) * pi)) * 1000
dens_j <- get_dens(trdat_j, sigma_j)
w_j <- mean_dens_j / dens_j
}
w_j[w_j > 1] <- 1
w_mat[trdat_ind, j] <- w_j
}
cm_mat[is.na(cm_mat)] <- 0
cm_sums <- apply(cm_mat, 1, sum)
w_cm_mat <- w_mat*cm_mat
w_cm_sums <- apply(
w_cm_mat,
1,
function(x) {
out <- sum(x, na.rm = TRUE)
return(out)
}
)
w_depth <- w_cm_sums / cm_sums
w_depth[!is.finite(w_depth)] <- 0
# Using the year as a covariate causes the model to identify the SINKS points
# based only on the sampling year, as the campaign took place over only two
# years, which were also underrepresented in the remaining training data.
if (frac == "SOC") {
w_year <- 0.99^(max(trdat$year, na.rm = TRUE) - trdat$year)
w_year %<>% { ifelse(is.na(.), 0, .) }
w_depth %<>% `*`(w_year)
}
weights_objects[[i]] <- list()
weights_objects[[i]]$cm_mat <- cm_mat
weights_objects[[i]]$cm_sums <- cm_sums
weights_objects[[i]]$w_cm_mat <- w_cm_mat
weights_objects[[i]]$w_cm_sums <- w_cm_sums
weights_objects[[i]]$w_depth <- w_depth
trdat$w <- w_depth
trdat_w_indices <- which(obs$ID_new %in% trdat$ID_new)
weights_objects[[i]]$indices <- trdat_w_indices
models_weights[trdat_w_indices, i] <- w_depth
# Three folds (placeholder)
trdat %<>% mutate(
fold = ceiling(fold / 3)
)
trdat %<>% filter(fold < 4)
if (!use_all_points) {
set.seed(1)
trdat %<>% sample_n(n)
}
trdat_indices <- which(obs$ID_new %in% trdat$ID_new)
models_indices[, i] <- obs$ID_new %in% trdat$ID_new
holdout_i <- obs[-trdat_indices, ]
holdout_indices <- which(obs$ID_new %in% holdout_i$ID_new)
# List of folds
folds_i <- lapply(
unique(trdat$fold),
function(x) {
out <- trdat %>%
mutate(
is_j = fold != x,
rnum = row_number(),
ind_j = is_j * rnum
) %>%
filter(ind_j != 0) %>%
dplyr::select(., ind_j) %>%
unlist() %>%
unname()
}
)
# Add depth boundaries and SOM removal as covariates
cov_keep_i <- c("upper", "lower")
if ( !(frac %in% c("SOC", "CaCO3")) ) {
cov_keep_i %<>% c(., "SOM_removed")
}
cov_c_i <- cov_selected %>% c(., cov_keep_i)
# Bayes optimization
bounds_pred_i <- c(bounds_lower[i], bounds_upper[i])
foreach::registerDoSEQ()
showConnections()
model_i <- optimize_xgboost(
target = frac,  # character vector (length 1), target variable.
cov_names = cov_c_i,  # Character vector, covariate names,
data = trdat, # data frame, input data
bounds_bayes = bounds, # named list with bounds for bayesian opt.
bounds_pred = bounds_pred_i, # numeric, length 2, bounds for predicted values
cores = 19, # number cores for parallelization
trgrid = tgrid, # data frame with tuning parameters to be tested in basic model
folds = folds_i, # list with indices, folds for cross validation
sumfun = sumfun_i, # summary function for accuracy assessment
metric = metrics[i], # character, length 1, name of evaluation metric
max_metric = FALSE, # logical, should the evaluation metric be maximized
weights = trdat$w, # numeric, weights for model training and evaluation
trees_per_round = 10, # numeric, length 1, number of trees that xgboost should train in each round
obj_xgb = objectives[i], # character, length 1, objective function for xgboost
colsample_bylevel_basic = 0.75, # numeric, colsample_bylevel for basic model
cov_keep = cov_keep_i, # Character vector, covariates that should always be present
final_round_mult = 10,  # Multiplier for the number of rounds in the final model
seed = 321,  # Random seed for model training,
classprob = FALSE
)
models_scoreresults[[i]] <- model_i$bayes_results
print(
models_scoreresults[[i]]$scoreSummary
)
models_bestscores[[i]] <- model_i$best_scores
print(
models_bestscores[[i]]
)
models[[i]] <- model_i$model_final
print(models[[i]])
# End of optimization
models_predictions[trdat_indices, i] <- models[[i]]$pred %>%
arrange(rowIndex) %>%
distinct(rowIndex, .keep_all = TRUE) %>%
dplyr::select(., pred) %>%
unlist() %>%
unname()
n_const_i <- length(cov_keep_i)
models_predictions[holdout_indices, i] <- predict_passna(
models[[i]],
holdout_i,
n_const = n_const_i
)
saveRDS(
models[[i]],
paste0(dir_results, "/model_", frac, ".rds")
)
}
# End of model training
names(weights_objects) <- fractions
saveRDS(
weights_objects,
paste0(dir_results, "/weights_objects.rds")
)
saveRDS(
models_scoreresults,
paste0(dir_results, "/models_scoreresults.rds")
)
saveRDS(
models_bestscores,
paste0(dir_results, "/models_bestscores.rds")
)
models_bestscores %>%
bind_rows() %>%
write.table(
file = paste0(dir_results, "/models_bestscores.csv"),
sep = ";",
row.names = FALSE
)
write.table(
models_weights,
file = paste0(dir_results, "/models_weights.csv"),
sep = ";",
row.names = FALSE
)
write.table(
models_indices,
file = paste0(dir_results, "/models_indices.csv"),
sep = ";",
row.names = FALSE
)
} else {
# Load existing models
models <- lapply(
1:length(fractions),
function(x) {
out <- fractions[x] %>%
paste0(dir_results, "/model_", ., ".rds") %>%
readRDS()
return(out)
}
)
weights_objects <- dir_results %>%
paste0(., "/weights_objects.rds") %>%
readRDS()
models_scoreresults <- dir_results %>%
paste0(., "/models_scoreresults.rds") %>%
readRDS()
models_bestscores <- dir_results %>%
paste0(., "/models_bestscores.rds") %>%
readRDS()
models_weights <- dir_results %>%
paste0(., "/models_weights.csv") %>%
read.table(
., header = TRUE,
sep = ";"
)
models_indices <- dir_results %>%
paste0(., "/models_indices.csv") %>%
read.table(
., header = TRUE,
sep = ";"
)
models_predictions <- dir_results %>%
paste0(., "/models_predictions_raw.csv") %>%
read.table(
., header = TRUE,
sep = ";"
)
}
