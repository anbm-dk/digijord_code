w_year %<>% { ifelse(is.na(.), 0, .) }
w_depth %<>% `*`(w_year)
}
weights_objects[[i]] <- list()
weights_objects[[i]]$cm_mat <- cm_mat
weights_objects[[i]]$cm_sums <- cm_sums
weights_objects[[i]]$w_cm_mat <- w_cm_mat
weights_objects[[i]]$w_cm_sums <- w_cm_sums
weights_objects[[i]]$w_depth <- w_depth
trdat$w <- w_depth
trdat_w_indices <- which(obs$ID_new %in% trdat$ID_new)
weights_objects[[i]]$indices <- trdat_w_indices
models_weights[trdat_w_indices, i] <- w_depth
# Three folds (placeholder)
trdat %<>% mutate(
fold = ceiling(fold / 3)
)
trdat %<>% filter(fold < 4)
if (!use_all_points) {
set.seed(1)
trdat %<>% sample_n(n)
}
trdat_indices <- which(obs$ID_new %in% trdat$ID_new)
models_indices[, i] <- obs$ID_new %in% trdat$ID_new
holdout_i <- obs[-trdat_indices, ]
holdout_indices <- which(obs$ID_new %in% holdout_i$ID_new)
# List of folds
folds_i <- lapply(
unique(trdat$fold),
function(x) {
out <- trdat %>%
mutate(
is_j = fold != x,
rnum = row_number(),
ind_j = is_j * rnum
) %>%
filter(ind_j != 0) %>%
dplyr::select(., ind_j) %>%
unlist() %>%
unname()
}
)
showConnections()
# Add depth boundaries and methods as covariates
cov_c_i <- cov_selected %>%
c("upper", "lower")
if (i %in% 1:4) {
cov_c_i <- cov_selected %>%
c("upper", "lower") %>%
c(., "SOM_removed")
}
# Identify covariates that are not OGCs
covs_not_ogc <- grep('ogc_pi', cov_c_i, value = TRUE, invert = TRUE)
cov_p_i <- covs_not_ogc %>% paste0(collapse = " + ")
formula_i <- paste0(frac, " ~ ", cov_p_i) %>%
as.formula()
# xgboost optimization
# 1: Fit learning rate (eta)
print("Step 1: Fit learning rate (eta)")
showConnections()
cl <- makePSOCKcluster(19)
registerDoParallel(cl)
clusterEvalQ(
cl,
{
library(boot)
}
)
clusterExport(
cl,
c(
"get_RMSEw",
"get_R2w"
)
)
set.seed(1)
models[[i]] <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = tgrid$nrounds,
eta = eta_test, # NB
max_depth = tgrid$max_depth,
min_child_weight = tgrid$min_child_weight,
gamma = tgrid$gamma,
colsample_bytree = tgrid$colsample_bytree,
subsample = tgrid$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = TRUE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i],
colsample_bylevel = 0.75,
nthread = 1
)
stopCluster(cl)
foreach::registerDoSEQ()
rm(cl)
models_tr_summaries[[i]][[tr_step]] <- models[[i]]$results
print(models_tr_summaries[[i]][[tr_step]])
tr_step %<>% `+`(1)
# Scaled cumulative covariate importance
cov_i_ranked <- varImp(models[[i]])$importance %>%
rownames_to_column() %>%
mutate(
scaled = Overall/sum(Overall),
cumul = cumsum(scaled)
)
if (extra_tuning_xgb & xgb_opt_stepwise) {
# CS Step 1: Drop unimportant covariates
cov_c_i <- cov_i_ranked %>%
filter(cumul < 0.99) %>%
.$rowname
cov_p_i <- cov_c_i %>% paste0(collapse = " + ")
formula_i <- paste0(frac, " ~ ", cov_p_i) %>%
as.formula()
# xgb opt Step 2: Fit max_depth and min_child_weight
print("Step 2: Fit max_depth and min_child_weight")
set.seed(1)
model2 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = models[[i]]$bestTune$nrounds,
eta = models[[i]]$bestTune$eta,
max_depth = max_depth_test, # NB
min_child_weight = min_child_weight_test, # NB
gamma = models[[i]]$bestTune$gamma,
colsample_bytree = models[[i]]$bestTune$colsample_bytree,
subsample = models[[i]]$bestTune$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
models[[i]] <- model2
models_tr_summaries[[i]][[tr_step]] <- models[[i]]$results
print(models_tr_summaries[[i]][[tr_step]])
tr_step %<>% `+`(1)
# xgb opt Step 3: Tune gamma
print("Step 3: Tune gamma")
set.seed(1)
model3 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = models[[i]]$bestTune$nrounds,
eta = models[[i]]$bestTune$eta,
max_depth = models[[i]]$bestTune$max_depth,
min_child_weight = models[[i]]$bestTune$min_child_weight,
gamma = gamma_test, # NB
colsample_bytree = models[[i]]$bestTune$colsample_bytree,
subsample = models[[i]]$bestTune$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
models[[i]] <- model3
models_tr_summaries[[i]][[tr_step]] <- models[[i]]$results
print(models_tr_summaries[[i]][[tr_step]])
tr_step %<>% `+`(1)
# xgb opt Step 4: Adjust subsampling
print("Step 4: Adjust subsampling")
set.seed(1)
model4 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = models[[i]]$bestTune$nrounds,
eta = models[[i]]$bestTune$eta,
max_depth = models[[i]]$bestTune$max_depth,
min_child_weight = models[[i]]$bestTune$min_child_weight,
gamma = models[[i]]$bestTune$gamma,
colsample_bytree = colsample_bytree_test,
subsample = subsample_test
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
models[[i]] <- model4
models_tr_summaries[[i]][[tr_step]] <- models[[i]]$results
print(models_tr_summaries[[i]][[tr_step]])
tr_step %<>% `+`(1)
# CS Step 2: Decide the optimal number of OGCs
cov_c_i <- varImp(models[[i]])$importance %>%
rownames_to_column() %>%
.$rowname
ogcs_names_list <- list(ogcs_names)
n_ogcs_v <- numeric()
m <- 1
n_ogcs <- length(ogcs_names_list[[m]])
n_ogcs_v[m] <- n_ogcs
while (n_ogcs > 2) {
m <- m + 1
ogcs_names_list[[m]] <- ogcs_names_list[[m - 1]][c(TRUE, FALSE)]
n_ogcs <- length(ogcs_names_list[[m]])
n_ogcs_v[m] <- n_ogcs
}
ogcs_names_list %<>% lapply(., function(x) {c(cov_c_i, x)})
ogcs_names_list[[length(ogcs_names_list) + 1]] <- cov_c_i
n_ogcs_v %<>% c(., 0)
print("Testing OGCs")
models_ogc_test <- ogcs_names_list %>%
lapply(
.,
function(x) {
cov_p_i <- x %>% paste0(collapse = " + ")
formula_i <- paste0(frac, " ~ ", cov_p_i) %>%
as.formula()
set.seed(1)
out <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = models[[i]]$bestTune$nrounds,
eta = models[[i]]$bestTune$eta,
max_depth = models[[i]]$bestTune$max_depth,
min_child_weight = models[[i]]$bestTune$min_child_weight,
gamma = models[[i]]$bestTune$gamma,
colsample_bytree = models[[i]]$bestTune$colsample_bytree,
subsample = models[[i]]$bestTune$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
return(out)
}
)
ogc_results <- models_ogc_test %>% lapply(
., function(x) x$results %>% select(any_of(metrics[i])) %>% min()
) %>%
unlist()
which_ogc_ind <- which.min(ogc_results)
ogc_df <- data.frame(
fraction = frac,
n_ogcs = n_ogcs_v,
acc = ogc_results,
metric = metrics[i]
)
write.table(
ogc_df,
paste0(dir_results, "ogc_acc_", frac, ".csv"),
row.names = FALSE,
col.names = TRUE,
sep = ";"
)
models_tr_summaries[[i]][[tr_step]] <- ogc_df
print(models_tr_summaries[[i]][[tr_step]])
tr_step %<>% `+`(1)
n_ogcs_models[i] <- n_ogcs_v[which_ogc_ind]
models[[i]] <- models_ogc_test[[which_ogc_ind]]
cov_c_i <- varImp(models_ogc_test[[which_ogc_ind]])$importance %>%
rownames_to_column() %>%
.$rowname
cov_p_i <- cov_c_i %>% paste0(collapse = " + ")
formula_i <- paste0(frac, " ~ ", cov_p_i) %>%
as.formula()
# xgb opt Step 5: Increase nrounds, readjust learning rate
print("Step 5")
eta_test_final <- model4$bestTune$eta %>%
log() %>%
seq(., . + log(0.01), length.out = 9) %>%
exp() %>%
round(3)
set.seed(1)
model5 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = models[[i]]$bestTune$nrounds*10,
eta = eta_test_final, # NB
max_depth = models[[i]]$bestTune$max_depth,
min_child_weight = models[[i]]$bestTune$min_child_weight,
gamma = models[[i]]$bestTune$gamma,
colsample_bytree = models[[i]]$bestTune$colsample_bytree,
subsample = models[[i]]$bestTune$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
models[[i]] <- model5
models_tr_summaries[[i]][[tr_step]] <- models[[i]]$results
print(models_tr_summaries[[i]][[tr_step]])
tr_step %<>% `+`(1)
}
# Bayes optimization
if (extra_tuning_xgb & !xgb_opt_stepwise) {
showConnections()
cl <- makeCluster(19)
registerDoParallel(cl)
clusterEvalQ(
cl,
{
library(caret)
library(xgboost)
library(magrittr)
library(dplyr)
library(tools)
library(boot)
}
)
bounds_lower_i <- bounds_lower[i]
bounds_upper_i <- bounds_upper[i]
metrics_i <- metrics[i]
objectives_i <- objectives[i]
clusterExport(
cl,
c("i",
"frac",
"bounds_lower_i",
"bounds_upper_i",
"cov_i_ranked",
"folds_i",
"get_RMSEw",
"get_R2w",
"metrics_i",
"objectives_i",
"ogcs_names_list",
"sumfun",
"tgrid",
"trdat",
"trees_per_round"
)
)
set.seed(321)
ScoreResult <- bayesOpt(
FUN = scoringFunction,
bounds = bounds,
initPoints = 19,
iters.n = 190,
iters.k = 19,
acq =  "ucb",
gsPoints = 190,
parallel = TRUE,
verbose = 1,
acqThresh = 0.95
)
stopCluster(cl)
foreach::registerDoSEQ()
rm(cl)
print(
ScoreResult$scoreSummary
)
print(
ScoreResult$scoreSummary[which.max(ScoreResult$scoreSummary$Score), ]
)
best_pars <- getBestPars(ScoreResult)
print("Training final model")
# Drop unimportant covariates
cov_i_filtered <- cov_i_ranked %>%
filter(cumul < best_pars$total_imp) %>%  #!
.$rowname
# Make sure SOM removal is a covariate
if (i %in% 1:4 & !"SOM_removed" %in% cov_i_filtered) {
cov_i_filtered %<>% c(., "SOM_removed")
}
total_imp_models[i] <- best_pars$total_imp
# Add OGCs
cov_i_filtered %<>% c(., ogcs_names_list[[best_pars$ogcs_index]])  # !
n_ogcs_models[i] <- n_ogcs_v[best_pars$ogcs_index]
# Make formula
cov_formula <- cov_i_filtered %>% paste0(collapse = " + ")
formula_i <- paste0(frac, " ~ ", cov_formula) %>%
as.formula()
# Lower eta
eta_test_final <- best_pars$eta %>%
log() %>%
seq(., . + log(0.01), length.out = 9) %>%
exp() %>%
round(3)
showConnections()
cl <- makePSOCKcluster(19)
registerDoParallel(cl)
clusterEvalQ(
cl,
{
library(boot)
}
)
clusterExport(
cl,
c(
"get_RMSEw",
"get_R2w"
)
)
set.seed(1)
model_final <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = tgrid$nrounds*10,
eta = eta_test_final, # NB
max_depth = best_pars$max_depth,
min_child_weight = best_pars$min_child_weight_sqrt^2,
gamma = best_pars$gamma_sqrt^2,
colsample_bytree = best_pars$colsample_bytree,
subsample = best_pars$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = TRUE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i],
colsample_bylevel = best_pars$colsample_bylevel,
nthread = 1
)
stopCluster(cl)
foreach::registerDoSEQ()
rm(cl)
models[[i]] <- model_final
}
print(models[[i]])
models_predictions[trdat_indices, i] <- models[[i]]$pred %>%
arrange(rowIndex) %>%
distinct(rowIndex, .keep_all = TRUE) %>%
dplyr::select(., pred) %>%
unlist() %>%
unname()
if (i %in% 1:4) {
n_const_i <- 3
} else {
n_const_i <- 2
}
models_predictions[holdout_indices, i] <- predict_passna(
models[[i]],
holdout_i,
n_const = n_const_i
)
saveRDS(
models[[i]],
paste0(dir_results, "/model_", frac, ".rds")
)
}
