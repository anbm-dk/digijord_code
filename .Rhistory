plot(
obs_top_v, "clay",
breaks = 5, breakby = "cases", col = cividis(5),
cex = 0.4
)
# 8: Set up models
cov_selected <- cov_cats %>%
filter(anbm_use == 1) %>%
dplyr::select(., name) %>%
unlist() %>%
unname()
# Template for custom eval
# evalerror <- function(preds, dtrain) {
#   labels <- getinfo(dtrain, "label")
#   err <- as.numeric(sum(labels != (preds > 0)))/length(labels)
#   return(list(metric = "error", value = err))
# }
# Weighted RMSE
get_RMSEw <- function(d, w) {
sqe <- w * (d[, 1] - d[, 2])^2
msqe <- sum(sqe) / sum(w)
out <- sqrt(msqe)
return(out)
}
# Weighted R^2
get_R2w <- function(d, w) {
require(boot)
out <- boot::corr(d[, 1:2], w)^2
return(out)
}
# Weighted summary function
WeightedSummary <- function(
data,
lev = NULL,
model = NULL,
...) {
out <- numeric()
out[1] <- get_RMSEw(data[, 1:2], data$weights)
out[2] <- get_R2w(data[, 1:2], data$weights)
names(out) <- c("RMSEw", "R2w")
return(out)
}
# Weighted summary function with log transformation
WeightedSummary_log <- function(
data,
lev = NULL,
model = NULL,
...) {
out <- numeric()
data[, 1:2] <- log(data[, 1:2])
data <- data[is.finite(rowSums(data)), ]
out[1] <- get_RMSEw(data[, 1:2], data$weights)
out[2] <- get_R2w(data[, 1:2], data$weights)
names(out) <- c("RMSEw_log", "R2w_log")
return(out)
}
# Weighted summary function with square root transformation
WeightedSummary_sqrt <- function(
data,
lev = NULL,
model = NULL,
...) {
out <- numeric()
data[, 1:2] <- sqrt(data[, 1:2])
data <- data[is.finite(rowSums(data)), ]
out[1] <- get_RMSEw(data[, 1:2], data$weights)
out[2] <- get_R2w(data[, 1:2], data$weights)
names(out) <- c("RMSEw_sqrt", "R2w_sqrt")
return(out)
}
metrics <- rep("RMSEw", length(fractions))
metrics[fractions == "SOC"] <- "RMSEw_log"
metrics[fractions == "CaCO3"] <- "RMSEw_sqrt"
# Function to calculate point densities
qnorm(seq(0.55, 0.95, 0.1), 0, 1)
get_dens <- function(datxy, sig) {
dens_out <- ppp(
datxy$UTMX,
datxy$UTMY,
c(441000, 894000),
c(6049000, 6403000)
) %>%
density(
sigma = sig,
at = "points",
leaveoneout = FALSE
)
attributes(dens_out) <- NULL
return(dens_out)
}
# Tuning grid
tgrid <- expand.grid(
nrounds = 10,
eta = 0.3,
max_depth = 6,
min_child_weight = 1,
gamma = 0,
colsample_bytree = 0.75,
subsample = 0.75
)
eta_test <- seq(0.1, 1, 0.1)
max_depth_test <- seq(1, 20, 3)
min_child_weight_test <- c(1, 2, 4, 8, 16, 32)
gamma_test <- seq(0, 0.6, 0.1)
colsample_bytree_test <- seq(0.5, 1.0, 0.1)
subsample_test <- seq(0.5, 1.0, 0.1)
objectives <- c(rep("reg:squarederror", 4), rep("reg:tweedie", 2))
trees_per_round <- 10
# Small random sample for testing
# Remember to include full dataset in the final model
n <- 1000
use_all_points <- TRUE
# use_all_points <- FALSE
extra_tuning_xgb <- TRUE
# extra_tuning_xgb <- FALSE
# 9: Train models
n_ogcs_models <- numeric()
weights_objects <- list()
# Covariate selection:
# Step 1: Decide the optimal number of OGCs
# Step 2: Drop unimportant covariates
# xgb optimization:
# Step 1: Adjust learning rate
# Step 2: Fit max_depth and min_child_weight
# Step 3: Tune gamma
# Step 4: Adjust subsampling
# Step 5: Increase nrounds, readjust learning rate
models <- list()
for (i in 1:length(fractions))
{
frac <- fractions[i]
print(frac)
if (metrics[i] == "RMSEw_log") {
sumfun <- WeightedSummary_log
} else {
if (metrics[i] == "RMSEw_sqrt") {
sumfun <- WeightedSummary_sqrt
} else {
sumfun <- WeightedSummary
}
}
trdat <- obs %>%
filter(is.finite(.data[[frac]])) %>%
filter(!is.na(UTMX) & !is.na(UTMY)) %>%
filter(lower > 0, upper < 200)
# Three folds (placeholder)
trdat %<>% mutate(
fold = ceiling(fold / 3)
)
holdout_i <- trdat %>%
filter(fold == 4)
trdat %<>% filter(fold < 4)
if (!use_all_points) {
set.seed(1)
trdat %<>% sample_n(n)
}
# Weighting by depth intervals
w_interval <- 10
w_increment <- 1
w_startdepth <- 0
w_maxdepth <- 200
w_depths <- seq(w_startdepth, w_maxdepth, w_increment)
w_iterations <- length(w_depths)
w_mat <- matrix(numeric(), nrow = nrow(trdat), ncol = w_iterations)
cm_mat <- matrix(numeric(), nrow = nrow(trdat), ncol = w_iterations)
for (j in 1:w_iterations)
{
upper_j <- w_depths[j] - w_interval
lower_j <- upper_j + w_interval * 2
trdat_ind <- trdat$lower > upper_j & trdat$upper < lower_j
trdat_ind[is.na(trdat_ind)] <- FALSE
trdat_j <- trdat[trdat_ind, ]
trdat_j %<>%
mutate(
thickness = lower - upper,
upper_int = case_when(
upper > upper_j ~ upper,
.default = upper_j
),
lower_int = case_when(
lower < lower_j ~ lower,
.default = lower_j
),
cm_int = case_when(
thickness == 0 ~ 1,
.default = lower_int - upper_int
)
)
cm_mat[trdat_ind, j] <- trdat_j$cm_int
# # Sigma equal to the radius of a circle with an equal area per sample
# sigma_j <- sqrt(43107 / (nrow(trdat_j) * pi)) * 1000
#
# # Use the expected mean density as a baseline
# mean_dens_j <- nrow(trdat_j) / (43107 * 10^6)
# For SOC:
# Separate densities for wetlands and uplands
if (frac == "SOC") {
areas <- c(39807, 3299)
w_j <- numeric(nrow(trdat_j))
for (k in 0:1) {
trdat_j_wl_ind <- trdat_j$cwl_10m_crisp == k
trdat_jk <- trdat_j[trdat_j_wl_ind, ]
# Sigma equal to the radius of a circle with an equal area per sample
sigma_jk <- sqrt(areas[k + 1] / (nrow(trdat_jk) * pi)) * 1000
# Use the expected mean density as a baseline
mean_dens_jk <- nrow(trdat_jk) / (areas[k + 1] * 10^6)
dens_jk <- get_dens(trdat_jk, sigma_jk)
w_j[trdat_j_wl_ind] <- mean_dens_jk / dens_jk
}
} else {
# Sigma equal to the radius of a circle with an equal area per sample
sigma_j <- sqrt(43107 / (nrow(trdat_j) * pi)) * 1000
# Use the expected mean density as a baseline
mean_dens_j <- nrow(trdat_j) / (43107 * 10^6)
dens_j <- get_dens(trdat_j, sigma_j)
w_j <- mean_dens_j / dens_j
}
w_j[w_j > 1] <- 1
w_mat[trdat_ind, j] <- w_j
}
cm_mat[is.na(cm_mat)] <- 0
cm_sums <- apply(cm_mat, 1, sum)
w_cm_mat <- w_mat*cm_mat
w_cm_sums <- apply(
w_cm_mat,
1,
function(x) {
out <- sum(x, na.rm = TRUE)
return(out)
}
)
w_depth <- w_cm_sums / cm_sums
w_depth[!is.finite(w_depth)] <- 0
# w_depth[w_depth > 1] <- 1
weights_objects[[i]] <- list()
weights_objects[[i]]$cm_mat <- cm_mat
weights_objects[[i]]$cm_sums <- cm_sums
weights_objects[[i]]$w_cm_mat <- w_cm_mat
weights_objects[[i]]$w_cm_sums <- w_cm_sums
weights_objects[[i]]$w_depth <- w_depth
trdat$w <- w_depth
# List of folds
folds_i <- lapply(
unique(trdat$fold),
function(x) {
out <- trdat %>%
mutate(
is_j = fold != x,
rnum = row_number(),
ind_j = is_j * rnum
) %>%
filter(ind_j != 0) %>%
dplyr::select(., ind_j) %>%
unlist() %>%
unname()
}
)
showConnections()
# Covariate selection
cov_c_i <- cov_selected %>%
c("upper", "lower")
if (i %in% 1:4) {
cov_c_i <- cov_selected %>%
c("upper", "lower") %>%
c(., "SOM_removed")
} else {
if (i == 5) {
cov_c_i <- cov_selected %>%
c("upper", "lower") %>%
c(., "year")
}
}
# CS Step 1: Decide the optimal number of OGCs
ogcs_names <- grep('ogc_pi', cov_c_i, value = TRUE)
covs_not_ogc <- grep('ogc_pi', cov_c_i, value = TRUE, invert = TRUE)
ogcs_names_list <- list(ogcs_names)
n_ogcs_v <- numeric()
m <- 1
n_ogcs <- length(ogcs_names_list[[m]])
n_ogcs_v[m] <- n_ogcs
while (n_ogcs > 2) {
m <- m + 1
ogcs_names_list[[m]] <- ogcs_names_list[[m - 1]][c(TRUE, FALSE)]
n_ogcs <- length(ogcs_names_list[[m]])
n_ogcs_v[m] <- n_ogcs
}
ogcs_names_list %<>% lapply(., function(x) {c(covs_not_ogc, x)})
ogcs_names_list[[length(ogcs_names_list) + 1]] <- covs_not_ogc
n_ogcs_v %<>% c(., 0)
print("Testing OGCs")
models_ogc_test <- ogcs_names_list %>%
lapply(
.,
function(x) {
cov_p_i <- x %>% paste0(collapse = " + ")
formula_i <- paste0(frac, " ~ ", cov_p_i) %>%
as.formula()
set.seed(1)
out <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = tgrid,
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
return(out)
}
)
ogc_results <- models_ogc_test %>% lapply(
., function(x) x$results %>% select(any_of(metrics[i])) %>% min()
) %>%
unlist()
which_ogc_ind <- which.min(ogc_results)
ogc_df <- data.frame(
fraction = frac,
n_ogcs = n_ogcs_v,
acc = ogc_results,
metric = metrics[i]
)
write.table(
ogc_df,
paste0(dir_results, "ogc_acc_", frac, ".csv"),
row.names = FALSE,
col.names = TRUE,
sep = ";"
)
n_ogcs_models[i] <- n_ogcs_v[which_ogc_ind]
# CS Step 2: Drop unimportant covariates
cov_c_i <- varImp(models_ogc_test[[which_ogc_ind]])$importance %>%
rownames_to_column() %>%
mutate(scaled = Overall/sum(Overall),
cumul = cumsum(scaled)) %>%
filter(cumul < 0.99) %>%
.$rowname
cov_p_i <- cov_c_i %>% paste0(collapse = " + ")
formula_i <- paste0(frac, " ~ ", cov_p_i) %>%
as.formula()
# xgboost optimization
# 1: Fit learning rate (eta) [and nrounds]
print("Step 1")
set.seed(1)
models[[i]] <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = tgrid$nrounds,
eta = eta_test, # NB
max_depth = tgrid$max_depth,
min_child_weight = tgrid$min_child_weight,
gamma = tgrid$gamma,
colsample_bytree = tgrid$colsample_bytree,
subsample = tgrid$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
# registerDoSEQ()
# rm(cl)
if (extra_tuning_xgb) {
# xgb opt Step 2: Fit max_depth and min_child_weight
print("Step 2")
set.seed(1)
model2 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = models[[i]]$bestTune$nrounds,
eta = models[[i]]$bestTune$eta,
max_depth = max_depth_test, # NB
min_child_weight = min_child_weight_test, # NB
gamma = models[[i]]$bestTune$gamma,
colsample_bytree = models[[i]]$bestTune$colsample_bytree,
subsample = models[[i]]$bestTune$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
# xgb opt Step 3: Tune gamma
print("Step 3")
set.seed(1)
model3 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = model2$bestTune$nrounds,
eta = model2$bestTune$eta,
max_depth = model2$bestTune$max_depth,
min_child_weight = model2$bestTune$min_child_weight,
gamma = gamma_test, # NB
colsample_bytree = model2$bestTune$colsample_bytree,
subsample = model2$bestTune$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
models[[i]] <- model3
# xgb opt Step 4: Adjust subsampling
print("Step 4")
set.seed(1)
model4 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = model3$bestTune$nrounds,
eta = model3$bestTune$eta,
max_depth = model3$bestTune$max_depth,
min_child_weight = model3$bestTune$min_child_weight,
gamma = model3$bestTune$gamma,
colsample_bytree = colsample_bytree_test,
subsample = subsample_test
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
# xgb opt Step 5: Increase nrounds, readjust learning rate
print("Step 5")
set.seed(1)
model5 <- caret::train(
form = formula_i,
data = trdat,
method = "xgbTree",
na.action = na.pass,
tuneGrid = expand.grid(
nrounds = model4$bestTune$nrounds*10,
eta = eta_test, # NB
max_depth = model4$bestTune$max_depth,
min_child_weight = model4$bestTune$min_child_weight,
gamma = model4$bestTune$gamma,
colsample_bytree = model4$bestTune$colsample_bytree,
subsample = model4$bestTune$subsample
),
trControl = trainControl(
index = folds_i,
savePredictions = "final",
predictionBounds = c(bounds_lower[i], bounds_upper[i]),
summaryFunction = sumfun,
allowParallel = FALSE
),
metric = metrics[i],
maximize = FALSE,
weights = trdat$w,
num_parallel_tree = trees_per_round,
objective = objectives[i]
)
models[[i]] <- model5
}
print(models[[i]])
saveRDS(
models[[i]],
paste0(dir_results, "/model_", frac, ".rds")
)
}
